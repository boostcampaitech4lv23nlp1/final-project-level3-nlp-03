{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import editdistance\n",
    "import transformers\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import LayoutLMv2FeatureExtractor\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bbox(bboxes, width, height):\n",
    "    a = [bboxes[0],bboxes[1],bboxes[4],bboxes[5]]\n",
    "    return [\n",
    "         int(1000*(a[0] / width)),\n",
    "         int(1000*(a[1] / height)),\n",
    "         int(1000*(a[2] / width)),\n",
    "         int(1000*(a[3] / height)),\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy(s1,s2):\n",
    "    return (editdistance.eval(s1,s2)/((len(s1)+len(s2))/2)) < 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocVQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,split):\n",
    "        datasets = load_dataset(\"Trailblazer-Yoo/boostcamp-docvqa\")\n",
    "        if split=='train':\n",
    "          self.dataset = datasets['train']\n",
    "        else:\n",
    "          self.dataset = datasets['val']\n",
    "\n",
    "        try:\n",
    "          model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n",
    "          self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "        except:\n",
    "          model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n",
    "          self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    \n",
    "    # source: https://stackoverflow.com/a/12576755\n",
    "    def subfinder(self, words_list, answer_list):  \n",
    "        matches = []\n",
    "        start_indices = []\n",
    "        end_indices = []\n",
    "        for idx, i in enumerate(range(len(words_list))):\n",
    "            #if words_list[i] == answer_list[0] and words_list[i:i+len(answer_list)] == answer_list:\n",
    "            if len(words_list[i:i+len(answer_list)])==len(answer_list) and all(fuzzy(words_list[i+j],answer_list[j]) for j in range(len(answer_list))):\n",
    "                matches.append(answer_list)\n",
    "                start_indices.append(idx)\n",
    "                end_indices.append(idx + len(answer_list) - 1)\n",
    "        if matches:\n",
    "          return matches[0], start_indices[0], end_indices[0]\n",
    "        else:\n",
    "          return None, 0, 0\n",
    "\n",
    "    def encode_dataset(self,example, max_length=512):\n",
    "          # take a batch \n",
    "          questions = example['question']\n",
    "          words = [w for w in example['words']] #handles numpy and list\n",
    "          boxes = example['boxes']\n",
    "\n",
    "          # encode it\n",
    "          encoding = self.tokenizer([questions], [words], [boxes], max_length=max_length, padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n",
    "          batch_index=0\n",
    "          input_ids = encoding.input_ids[batch_index].tolist()\n",
    "\n",
    "          # next, add start_positions and end_positions\n",
    "          start_positions = []\n",
    "          end_positions = []\n",
    "          answers = example['answers']\n",
    "          #print(\"Batch index:\", batch_index)\n",
    "          cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
    "          # try to find one of the answers in the context, return first match\n",
    "          words_example = [word.lower() for word in words]\n",
    "          for answer in answers:\n",
    "            match, word_idx_start, word_idx_end = self.subfinder(words_example, answer.lower().split())\n",
    "            #if match:\n",
    "            #  break\n",
    "            # EXPERIMENT (to account for when OCR context and answer don't perfectly match):\n",
    "            if not match and len(answer)>1:\n",
    "                for i in range(len(answer)):\n",
    "                  # drop the ith character from the answer\n",
    "                  answer_i = answer[:i] + answer[i+1:]\n",
    "                  # check if we can find this one in the context\n",
    "                  match, word_idx_start, word_idx_end = self.subfinder(words_example, answer_i.lower().split())\n",
    "                  if match:\n",
    "                    break\n",
    "            # END OF EXPERIMENT \n",
    "            if match:\n",
    "              sequence_ids = encoding.sequence_ids(batch_index)\n",
    "              # Start token index of the current span in the text.\n",
    "              token_start_index = 0\n",
    "              while sequence_ids[token_start_index] != 1:\n",
    "                  token_start_index += 1\n",
    "\n",
    "              # End token index of the current span in the text.\n",
    "              token_end_index = len(input_ids) - 1\n",
    "              while sequence_ids[token_end_index] != 1:\n",
    "                  token_end_index -= 1\n",
    "              \n",
    "              word_ids = encoding.word_ids(batch_index)[token_start_index:token_end_index+1]\n",
    "\n",
    "              hit=False\n",
    "              for id in word_ids:\n",
    "                if id == word_idx_start:\n",
    "                  start_positions.append(token_start_index)\n",
    "                  hit=True\n",
    "                  break\n",
    "                else:\n",
    "                  token_start_index += 1\n",
    "\n",
    "              if not hit:\n",
    "                  continue\n",
    "        \n",
    "              hit=False\n",
    "              for id in word_ids[::-1]:\n",
    "                if id == word_idx_end:\n",
    "                  end_positions.append(token_end_index)\n",
    "                  hit=True\n",
    "                  break\n",
    "                else:\n",
    "                  token_end_index -= 1\n",
    "\n",
    "              if not hit:\n",
    "                  end_positions.append(token_end_index)\n",
    "              \n",
    "              #print(\"Verifying start position and end position:\")\n",
    "              #print(\"True answer:\", answer)\n",
    "              #start_position = start_positions[-1]\n",
    "              #end_position = end_positions[-1]\n",
    "              #reconstructed_answer = tokenizer.decode(encoding.input_ids[batch_index][start_position:end_position+1])\n",
    "              #print(\"Reconstructed answer:\", reconstructed_answer)\n",
    "              #print(\"-----------\")\n",
    "            \n",
    "            #else:\n",
    "              #print(\"Answer not found in context\")\n",
    "              #print(\"-----------\")\n",
    "              #start_positions.append(cls_index)\n",
    "              #end_positions.append(cls_index)\n",
    "\n",
    "          if len(start_positions)==0:\n",
    "              return None\n",
    "        \n",
    "          ans_i = random.randrange(len(start_positions))\n",
    "\n",
    "          encoding = {\n",
    "                  'input_ids': encoding['input_ids'],\n",
    "                  'attention_mask': encoding['attention_mask'],\n",
    "                  'token_type_ids': encoding['token_type_ids'],\n",
    "                  'bbox': encoding['bbox'],\n",
    "                  'answers' : answers\n",
    "                  }\n",
    "          ## 바뀐 부분 example['image'].copy() -> example['image'].copy()[0]\n",
    "          encoding['image'] = torch.LongTensor(example['image'].copy()[0])\n",
    "          encoding['start_position'] = torch.LongTensor([start_positions[ans_i]])\n",
    "          encoding['end_position'] = torch.LongTensor([end_positions[ans_i]])\n",
    "\n",
    "          return encoding\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "      data = self.dataset[index]\n",
    "      data = self.encode_dataset(data)\n",
    "\n",
    "      if data is None:\n",
    "                #return self.__getitem__((index+1)%len(self))\n",
    "        index = random.randrange(len(self))\n",
    "        return self.__getitem__(index)\n",
    "\n",
    "      return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data):\n",
    "    return {\n",
    "            'input_ids': torch.cat([d['input_ids'] for d in data],dim=0),\n",
    "            'attention_mask': torch.cat([d['attention_mask'] for d in data],dim=0),\n",
    "            'token_type_ids': torch.cat([d['token_type_ids'] for d in data],dim=0),\n",
    "            'bbox': torch.cat([d['bbox'] for d in data],dim=0),\n",
    "            'image': torch.stack([d['image'] for d in data],dim=0),\n",
    "            'start_positions': torch.cat([d['start_position'] for d in data],dim=0),\n",
    "            'end_positions': torch.cat([d['end_position'] for d in data],dim=0),\n",
    "            'answers': [d['answers'] for d in data],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv2Processor\n",
    "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f640448381aa41f3b556b53c49a4f45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/802M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForQuestionAnswering: ['layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked']\n",
      "- This IS expected if you are initializing LayoutLMv2ForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMv2ForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LayoutLMv2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias', 'layoutlmv2.visual_segment_embedding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoModel\n",
    "from transformers import AdamW\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "optimizer = AdamW(model.parameters(),lr=5e-5)\n",
    "start_epoch=0\n",
    "start_idx=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layoutlmv2.encoder.layer.0.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.1.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.2.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.3.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.4.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.5.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.6.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.7.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.8.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.9.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.10.attention.self.dropout\n",
      "layoutlmv2.encoder.layer.11.attention.self.dropout\n"
     ]
    }
   ],
   "source": [
    "for name, modele in model.named_modules():\n",
    "    if 'self.dropout' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  steps = 0\n",
    "  pbar = tqdm(dataloader)  # loop over the dataset multiple times\n",
    "  for idx, batch in enumerate(pbar):\n",
    "      \n",
    "      input_ids = batch[\"input_ids\"].to(device)\n",
    "      attention_mask = batch[\"attention_mask\"].to(device)\n",
    "      token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "      bbox = batch[\"bbox\"].to(device)\n",
    "      image = batch[\"image\"].to(device)\n",
    "      start_positions = batch[\"start_positions\"].to(device)\n",
    "      end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "      steps += 1\n",
    "        # forward + backward + optimize\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                       bbox=bbox, image=image, \n",
    "                       start_positions=start_positions, \n",
    "                       end_positions=end_positions\n",
    "                       )\n",
    "      loss = outputs.loss\n",
    "      epoch_loss += loss.detach().cpu().numpy().item()\n",
    "      #   print(\"Loss:\", loss.item())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      pbar.set_postfix({\n",
    "                'loss' : epoch_loss / steps,\n",
    "                'lr' : optimizer.param_groups[0]['lr'],\n",
    "            })\n",
    "      wandb.log({'train_loss':epoch_loss/steps})\n",
    "  # torch.save(model.state_dict(), '/opt/ml/docvqa/model.pt')\n",
    "  pbar.close()\n",
    "\n",
    "def ANLS(pred,answers):\n",
    "    if answers[0] is not None:\n",
    "        scores = []\n",
    "        for ans in answers:\n",
    "            ed = editdistance.eval(ans.lower(),pred.lower())\n",
    "            NL = ed/max(len(ans),len(pred))\n",
    "            scores.append(1-NL if NL<0.5 else 0)\n",
    "        return max(scores)\n",
    "    return []\n",
    "\n",
    "def run (batch, start_logits,end_logits):\n",
    "    batch_score = 0\n",
    "    length = len(batch['input_ids'])\n",
    "    for i in range(length):\n",
    "        predicted_start_idx = start_logits[i].argmax(-1).item()\n",
    "        predicted_end_idx = end_logits[i].argmax(-1).item()\n",
    "        try:\n",
    "            valid = processor.tokenizer.decode(batch['input_ids'][i][predicted_start_idx:predicted_end_idx+1])\n",
    "            batch_score += ANLS(valid, batch['answers'][i])\n",
    "        except:\n",
    "            continue\n",
    "    return batch_score / length\n",
    "\n",
    "min_loss = 1\n",
    "score = 0\n",
    "def valid_epoch(epoch):\n",
    "    global min_loss\n",
    "    global score\n",
    "    model.eval()\n",
    "    gc.collect()\n",
    "    start_logits_all, end_logits_all = [], []\n",
    "    epoch_val_loss = 0\n",
    "    steps = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(valid_dataloader)\n",
    "        length = len(valid_dataloader)\n",
    "        for idx, batch in enumerate(pbar):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            bbox = batch[\"bbox\"].to(device)\n",
    "            image = batch[\"image\"].to(device)\n",
    "            start_positions = batch[\"start_positions\"].to(device)\n",
    "            end_positions = batch[\"end_positions\"].to(device)            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                       bbox=bbox, image=image, \n",
    "                       start_positions=start_positions, \n",
    "                       end_positions=end_positions\n",
    "                       )\n",
    "            steps += 1\n",
    "            loss = outputs.loss\n",
    "            epoch_val_loss += loss.detach().cpu().numpy().item()\n",
    "            batch_score = run(batch, outputs.start_logits, outputs.end_logits)\n",
    "            score += batch_score\n",
    "            pbar.set_postfix({\n",
    "                'val_loss' : epoch_val_loss / steps,\n",
    "                'val_score' : batch_score,\n",
    "                'lr' : optimizer.param_groups[0]['lr'],\n",
    "            })\n",
    "    epoch_val_loss /= steps\n",
    "    score /= steps\n",
    "    # start_logits_all = np.concatenate(start_logits_all)[:length]\n",
    "    # end_logits_all = np.concatenate(end_logits_all)[:length]\n",
    "    print(f\"Epoch [{epoch+1}] Val_loss : {epoch_val_loss}\")\n",
    "    print(f\"Epoch [{epoch+1}] Val_Score : {score}\" )\n",
    "    wandb.log({'epoch' : epoch+1, 'val_loss' : epoch_val_loss})\n",
    "    wandb.log({'epoch' : epoch+1, 'val_score' : score})\n",
    "\n",
    "    if epoch_val_loss < min_loss:\n",
    "        torch.save(model.state_dict(), '/opt/ml/docvqa/model.pt')\n",
    "        min_loss = epoch_val_loss\n",
    "\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Trailblazer-Yoo--boostcamp-docvqa-36fdb9dc869c2269\n",
      "Found cached dataset parquet (/opt/ml/.cache/huggingface/datasets/Trailblazer-Yoo___parquet/Trailblazer-Yoo--boostcamp-docvqa-36fdb9dc869c2269/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 238.71it/s]\n",
      "Using custom data configuration Trailblazer-Yoo--boostcamp-docvqa-36fdb9dc869c2269\n",
      "Found cached dataset parquet (/opt/ml/.cache/huggingface/datasets/Trailblazer-Yoo___parquet/Trailblazer-Yoo--boostcamp-docvqa-36fdb9dc869c2269/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 269.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "dataset = DocVQADataset('train')\n",
    "valid_dataset = DocVQADataset('valid')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16,collate_fn=collate, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=16,collate_fn=collate, shuffle=False)\n",
    "\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.init(entity='hundredeuk2',\n",
    "                project='LayoutLM',\n",
    "                group='test_QA',\n",
    "                name='test',\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    train_epoch()\n",
    "    valid_epoch(i)\n",
    "    gc.collect()\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
